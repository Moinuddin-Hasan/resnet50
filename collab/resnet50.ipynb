{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYrpWAe/UrAmdvQ9EBMHsE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moinuddin-Hasan/resnet50/blob/master/resnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa8QfljyMOan"
      },
      "outputs": [],
      "source": [
        "# Install tqdm for progress bars\n",
        "!pip install -q tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For downloading the dataset\n",
        "import requests\n",
        "import tarfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "config = {\n",
        "    \"DATA_PATH\": \"/content/imagenette2-320\",\n",
        "    \"MODEL_NAME\": \"resnet50\",\n",
        "    \"NUM_CLASSES\": 10,  # Imagenette has 10 classes\n",
        "    \"BATCH_SIZE\": 64,   # Adjust based on Colab GPU memory\n",
        "    \"NUM_EPOCHS\": 10,   # Number of epochs for this test run\n",
        "    \"LR\": 0.01,         # Initial Learning Rate\n",
        "    \"MOMENTUM\": 0.9,\n",
        "    \"WEIGHT_DECAY\": 1e-4,\n",
        "    \"LR_STEP_SIZE\": 5,  # Decay LR every 5 epochs\n",
        "    \"LR_GAMMA\": 0.1,    # LR decay factor\n",
        "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"CHECKPOINT_PATH\": \"outputs/checkpoints\"\n",
        "}\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(config[\"CHECKPOINT_PATH\"], exist_ok=True)\n",
        "\n",
        "print(f\"Using device: {config['DEVICE']}\")"
      ],
      "metadata": {
        "id": "5x0Y1jjlMUPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Dataset Preparation ---\n",
        "def download_and_extract_imagenette():\n",
        "    url = \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\"\n",
        "    target_path = '/content/imagenette2-320.tgz'\n",
        "    extract_path = '/content/'\n",
        "\n",
        "    if os.path.exists(config[\"DATA_PATH\"]):\n",
        "        print(\"Dataset already downloaded and extracted.\")\n",
        "        return\n",
        "\n",
        "    print(\"Downloading Imagenette...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(target_path, \"wb\") as f:\n",
        "        f.write(response.raw.read())\n",
        "\n",
        "    print(\"Extracting dataset...\")\n",
        "    with tarfile.open(target_path, \"r:gz\") as tar:\n",
        "        tar.extractall(path=extract_path)\n",
        "\n",
        "    print(\"Dataset ready.\")\n",
        "\n",
        "download_and_extract_imagenette()"
      ],
      "metadata": {
        "id": "PLesJqJVMV91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Augmentation and Loaders ---\n",
        "\n",
        "# Input size for ResNet is typically 224x224\n",
        "input_size = 224\n",
        "\n",
        "# Normalization values for ImageNet\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# Data augmentation for training\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(input_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "# Just resize, center crop, and normalize for validation\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(input_size + 32), # Resize to 256\n",
        "    transforms.CenterCrop(input_size), # Center crop to 224\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ImageFolder(os.path.join(config[\"DATA_PATH\"], 'train'), transform=train_transform)\n",
        "val_dataset = ImageFolder(os.path.join(config[\"DATA_PATH\"], 'val'), transform=val_transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Found {len(train_dataset)} training images in {len(train_dataset.classes)} classes.\")\n",
        "print(f\"Found {len(val_dataset)} validation images in {len(val_dataset.classes)} classes.\")"
      ],
      "metadata": {
        "id": "C9GwINhCMYKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Definition ---\n",
        "\n",
        "# Load a ResNet50 model but do NOT use pre-trained weights\n",
        "model = torchvision.models.resnet50(weights=None, num_classes=config[\"NUM_CLASSES\"])\n",
        "\n",
        "# Move the model to the configured device (GPU)\n",
        "model = model.to(config[\"DEVICE\"])\n",
        "\n",
        "# Print model summary (optional)\n",
        "# from torchsummary import summary\n",
        "# summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "kGA5snz4Mb10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Components ---\n",
        "\n",
        "# Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (SGD with Momentum)\n",
        "optimizer = optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=config[\"LR\"],\n",
        "    momentum=config[\"MOMENTUM\"],\n",
        "    weight_decay=config[\"WEIGHT_DECAY\"]\n",
        ")\n",
        "\n",
        "# Learning Rate Scheduler (Step Decay)\n",
        "scheduler = StepLR(\n",
        "    optimizer,\n",
        "    step_size=config[\"LR_STEP_SIZE\"],\n",
        "    gamma=config[\"LR_GAMMA\"]\n",
        ")"
      ],
      "metadata": {
        "id": "JUXuwlGBMeGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training and Validation Logic ---\n",
        "\n",
        "def train_one_epoch(model, criterion, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Wrap data_loader with tqdm for a progress bar\n",
        "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{config['NUM_EPOCHS']} [T]\")\n",
        "    for inputs, labels in progress_bar:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Statistics\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_samples += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix(loss=total_loss/total_samples, acc=f\"{(100*correct_predictions/total_samples):.2f}%\")\n",
        "\n",
        "    epoch_loss = total_loss / total_samples\n",
        "    epoch_acc = correct_predictions / total_samples\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate(model, criterion, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad(): # No gradients needed for validation\n",
        "        progress_bar = tqdm(data_loader, desc=\"Validating\")\n",
        "        for inputs, labels in progress_bar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "            progress_bar.set_postfix(loss=total_loss/total_samples, acc=f\"{(100*correct_predictions/total_samples):.2f}%\")\n",
        "\n",
        "    epoch_loss = total_loss / total_samples\n",
        "    epoch_acc = correct_predictions / total_samples\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "Jpc7VEZJMf4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Training Loop ---\n",
        "\n",
        "best_val_acc = 0.0\n",
        "\n",
        "print(\"Starting training...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(config[\"NUM_EPOCHS\"]):\n",
        "    # --- Train ---\n",
        "    train_loss, train_acc = train_one_epoch(model, criterion, optimizer, train_loader, config[\"DEVICE\"], epoch)\n",
        "\n",
        "    # --- Validate ---\n",
        "    val_loss, val_acc = validate(model, criterion, val_loader, config[\"DEVICE\"])\n",
        "\n",
        "    # --- Log Results ---\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{config['NUM_EPOCHS']} | \"\n",
        "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "    # --- Step the scheduler ---\n",
        "    scheduler.step()\n",
        "\n",
        "    # --- Save the best model ---\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_path = os.path.join(config[\"CHECKPOINT_PATH\"], \"best_model.pth\")\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"New best model saved to {best_model_path} with accuracy: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training finished in {(end_time - start_time)/60:.2f} minutes.\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "pIdLPkDPMhp8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}